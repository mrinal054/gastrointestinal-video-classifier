{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100",
      "authorship_tag": "ABX9TyPWJJ/9Jw6n6zO0nF2lc2JE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mrinal054/gastrointestinal-video-classifier/blob/main/GI_video_classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gastrointestinal (GI) Tract Video Classifier"
      ],
      "metadata": {
        "id": "J-XymlcR3JpR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset used: HyperKvasir (Link: https://osf.io/mh9sj/)"
      ],
      "metadata": {
        "id": "p5_lMiiB3jij"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WnMTHYdqsAEz"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "sys.path.append('/content/drive/MyDrive/Video_classification/my_utils')\n",
        "sys.path.append('/content/drive/MyDrive/Video_classification')\n",
        "sys.path.append('/content/drive/MyDrive/library')"
      ],
      "metadata": {
        "id": "CYjd9BR3sLVV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tqdm\n",
        "import random\n",
        "import itertools\n",
        "import collections\n",
        "import os\n",
        "import random\n",
        "import pathlib\n",
        "from pathlib import PosixPath # for windows, import Path; for linux, import PosixPath\n",
        "\n",
        "import cv2\n",
        "import einops\n",
        "import numpy as np\n",
        "import remotezip as rz\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras import layers\n",
        "from my_utils import ucf\n",
        "\n",
        "# Import packages and libraries\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, CSVLogger, TerminateOnNaN\n",
        "import pandas as pd # to read/write excel or csv file\n",
        "from matplotlib import pyplot as plt # to plot or visualize data\n",
        "from datetime import datetime\n",
        "from copy import deepcopy\n"
      ],
      "metadata": {
        "id": "k_3CUeBTsOtI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "HEIGHT = 224\n",
        "WIDTH = 224\n",
        "n_classes = 2\n",
        "seed = random.randint(1, 1000) # this seed will be used to randomly pick videos in the later section\n",
        "\n",
        "print(f'seed: {seed}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XOEknFOsdFNw",
        "outputId": "174d9afc-049b-4309-851e-6bd472c11d19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "seed: 506\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model name\n",
        "model_name = 'ResNet_PscSE_seed' + str(seed) + '_' + datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
        "print(model_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gcgh0xK0YUxn",
        "outputId": "6695e577-02b1-442a-e147-b0a312d27bd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ResNet_seed506_2023-11-17_15-04-51\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(2 + 1)D convolution"
      ],
      "metadata": {
        "id": "W5LjxiU469Jk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Conv2Plus1D(keras.layers.Layer):\n",
        "  def __init__(self, filters, kernel_size, padding):\n",
        "    \"\"\"\n",
        "      A sequence of convolutional layers that first apply the convolution operation over the\n",
        "      spatial dimensions, and then the temporal dimension.\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "    self.seq = keras.Sequential([\n",
        "        # Spatial decomposition\n",
        "        layers.Conv3D(filters=filters,\n",
        "                      kernel_size=(1, kernel_size[1], kernel_size[2]),\n",
        "                      padding=padding),\n",
        "        # Temporal decomposition\n",
        "        layers.Conv3D(filters=filters,\n",
        "                      kernel_size=(kernel_size[0], 1, 1),\n",
        "                      padding=padding)\n",
        "        ])\n",
        "\n",
        "  def call(self, x):\n",
        "    return self.seq(x)"
      ],
      "metadata": {
        "id": "GwLMXrU66ZmR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the main branch of the residual block with the following class. In contrast to the standard ResNet structure this uses the custom Conv2Plus1D layer instead of layers.Conv2D."
      ],
      "metadata": {
        "id": "s1SFW2M76uw7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualMain(keras.layers.Layer):\n",
        "  \"\"\"\n",
        "    Residual block of the model with convolution, layer normalization, and the\n",
        "    activation function, ReLU.\n",
        "  \"\"\"\n",
        "  def __init__(self, filters, kernel_size):\n",
        "    super().__init__()\n",
        "    self.seq = keras.Sequential([\n",
        "        Conv2Plus1D(filters=filters,\n",
        "                    kernel_size=kernel_size,\n",
        "                    padding='same'),\n",
        "        layers.LayerNormalization(),\n",
        "        layers.ReLU(),\n",
        "        Conv2Plus1D(filters=filters,\n",
        "                    kernel_size=kernel_size,\n",
        "                    padding='same'),\n",
        "        layers.LayerNormalization()\n",
        "    ])\n",
        "\n",
        "  def call(self, x):\n",
        "    return self.seq(x)"
      ],
      "metadata": {
        "id": "g8B7Bxhj6ciY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To add the residual branch to the main branch it needs to have the same size. The Project layer below deals with cases where the number of channels is changed on the branch. In particular, a sequence of densely-connected layer followed by normalization is added."
      ],
      "metadata": {
        "id": "WlfoQOs17LPQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Project(keras.layers.Layer):\n",
        "  \"\"\"\n",
        "    Project certain dimensions of the tensor as the data is passed through different\n",
        "    sized filters and downsampled.\n",
        "  \"\"\"\n",
        "  def __init__(self, units):\n",
        "    super().__init__()\n",
        "    self.seq = keras.Sequential([\n",
        "        layers.Dense(units),\n",
        "        layers.LayerNormalization()\n",
        "    ])\n",
        "\n",
        "  def call(self, x):\n",
        "    return self.seq(x)"
      ],
      "metadata": {
        "id": "DIOwAcpenlJK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use add_residual_block to introduce a skip connection between the layers of the model."
      ],
      "metadata": {
        "id": "xZPCMORV7O5H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_residual_block(input, filters, kernel_size):\n",
        "  \"\"\"\n",
        "    Add residual blocks to the model. If the last dimensions of the input data\n",
        "    and filter size does not match, project it such that last dimension matches.\n",
        "  \"\"\"\n",
        "  out = ResidualMain(filters,\n",
        "                     kernel_size)(input)\n",
        "\n",
        "  res = input\n",
        "  # Using the Keras functional APIs, project the last dimension of the tensor to\n",
        "  # match the new filter size\n",
        "  if out.shape[-1] != input.shape[-1]:\n",
        "    res = Project(out.shape[-1])(res)\n",
        "\n",
        "  return layers.add([res, out])"
      ],
      "metadata": {
        "id": "RZ2gcVcinpUB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Resizing the video is necessary to perform downsampling of the data. In particular, downsampling the video frames allow for the model to examine specific parts of frames to detect patterns that may be specific to a certain action. Through downsampling, non-essential information can be discarded. Moreoever, resizing the video will allow for dimensionality reduction and therefore faster processing through the model."
      ],
      "metadata": {
        "id": "oEkFZGU27WBI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResizeVideo(keras.layers.Layer):\n",
        "  def __init__(self, height, width):\n",
        "    super().__init__()\n",
        "    self.height = height\n",
        "    self.width = width\n",
        "    self.resizing_layer = layers.Resizing(self.height, self.width)\n",
        "\n",
        "  def call(self, video):\n",
        "    \"\"\"\n",
        "      Use the einops library to resize the tensor.\n",
        "\n",
        "      Args:\n",
        "        video: Tensor representation of the video, in the form of a set of frames.\n",
        "\n",
        "      Return:\n",
        "        A downsampled size of the video according to the new height and width it should be resized to.\n",
        "    \"\"\"\n",
        "    # b stands for batch size, t stands for time, h stands for height,\n",
        "    # w stands for width, and c stands for the number of channels.\n",
        "    old_shape = einops.parse_shape(video, 'b t h w c')\n",
        "    images = einops.rearrange(video, 'b t h w c -> (b t) h w c')\n",
        "    images = self.resizing_layer(images)\n",
        "    videos = einops.rearrange(\n",
        "        images, '(b t) h w c -> b t h w c',\n",
        "        t = old_shape['t'])\n",
        "    return videos"
      ],
      "metadata": {
        "id": "97_v4kSIntkR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build 3D P-scSE"
      ],
      "metadata": {
        "id": "XweYwaRvnuwx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "class Conv3dReLU(keras.layers.Layer):\n",
        "    def __init__(\n",
        "        self,\n",
        "        out_channels,\n",
        "        kernel_size,\n",
        "        padding='same',\n",
        "        stride=1,\n",
        "        use_batchnorm=True,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.seq = keras.Sequential([\n",
        "            layers.Conv3D(\n",
        "                out_channels,\n",
        "                kernel_size,\n",
        "                strides=stride,\n",
        "                padding=padding,\n",
        "                use_bias=not use_batchnorm,\n",
        "            ),\n",
        "            layers.BatchNormalization(),\n",
        "            layers.ReLU(),\n",
        "        ])\n",
        "\n",
        "    def call(self, x):\n",
        "      return self.seq(x)\n",
        "\n",
        "#%%\n",
        "class SCSEModule0(keras.layers.Layer):\n",
        "    def __init__(self, in_channels, reduction=16, strategy='addition'):\n",
        "        super().__init__()\n",
        "        self.strategy = strategy\n",
        "\n",
        "        self.cSE = keras.Sequential([\n",
        "            layers.GlobalAveragePooling3D(keepdims=True),\n",
        "            layers.Conv3D(in_channels // reduction, 1, activation='relu'),\n",
        "            layers.Conv3D(in_channels, 1, activation='sigmoid'),\n",
        "        ])\n",
        "\n",
        "        self.sSE = keras.Sequential([\n",
        "            layers.Conv3D(1, 1, activation='sigmoid'),\n",
        "        ])\n",
        "\n",
        "    def call(self, x):\n",
        "\n",
        "        x_cSE = self.cSE(x)\n",
        "        xc = x * x_cSE      # cSE attention\n",
        "\n",
        "        x_sSE = self.sSE(x)\n",
        "        x_sSE = layers.Concatenate(axis=-1)([x_sSE] * x.shape[-1])\n",
        "        xs = x * x_sSE      # sSE\n",
        "\n",
        "        if self.strategy == 'addition':\n",
        "          return xc + xs\n",
        "\n",
        "        elif self.strategy == 'maxout':\n",
        "            return tf.maximum(xc, xs)\n",
        "\n",
        "        else:\n",
        "            raise ValueError(\"Wrong keyword for attention strategy. Choose from [addition, maxout]\")\n",
        "\n",
        "#%%\n",
        "class PscSE3D(keras.layers.Layer):\n",
        "    def __init__(\n",
        "        self,\n",
        "        channels, # in_channels and out_channels will be equal\n",
        "        reduction,\n",
        "        use_batchnorm=True,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1 = Conv3dReLU(\n",
        "            out_channels=channels, # out_channels will be equal to in_channels\n",
        "            kernel_size=3,\n",
        "            padding='same',\n",
        "            use_batchnorm=use_batchnorm,\n",
        "        )\n",
        "\n",
        "        # P-scSE\n",
        "        self.attention1 = SCSEModule0(in_channels=channels, reduction=reduction, strategy='maxout')\n",
        "        self.attention2 = SCSEModule0(in_channels=channels, reduction=reduction, strategy='addition')\n",
        "\n",
        "    def call(self, x):\n",
        "        x1 = self.attention1(x)  # 1st attention\n",
        "        x2 = self.attention2(x)  # 2nd attention\n",
        "\n",
        "        x3 = x1 + x2\n",
        "\n",
        "        x3 = self.conv1(x3)\n",
        "        return x3"
      ],
      "metadata": {
        "id": "FnwnAWpbtD9q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use the Keras functional API to build the residual network."
      ],
      "metadata": {
        "id": "pTVUr2xb7prQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_shape = (None, 10, HEIGHT, WIDTH, 3)\n",
        "input = layers.Input(shape=(input_shape[1:]))\n",
        "x = input\n",
        "\n",
        "x = Conv2Plus1D(filters=16, kernel_size=(3, 7, 7), padding='same')(x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.ReLU()(x)\n",
        "x = ResizeVideo(HEIGHT // 2, WIDTH // 2)(x)\n",
        "\n",
        "# Block 1\n",
        "x = add_residual_block(x, 16, (3, 3, 3))\n",
        "x = PscSE3D(channels=16, reduction=8)(x)\n",
        "x = ResizeVideo(HEIGHT // 4, WIDTH // 4)(x)\n",
        "\n",
        "# Block 2\n",
        "x = add_residual_block(x, 32, (3, 3, 3))\n",
        "x = PscSE3D(channels=32, reduction=8)(x)\n",
        "x = ResizeVideo(HEIGHT // 8, WIDTH // 8)(x)\n",
        "\n",
        "# Block 3\n",
        "x = add_residual_block(x, 64, (3, 3, 3))\n",
        "x = PscSE3D(channels=64, reduction=8)(x)\n",
        "x = ResizeVideo(HEIGHT // 16, WIDTH // 16)(x)\n",
        "\n",
        "# Block 4\n",
        "x = add_residual_block(x, 128, (3, 3, 3))\n",
        "x = PscSE3D(channels=128, reduction=8)(x)\n",
        "\n",
        "x = layers.GlobalAveragePooling3D()(x)\n",
        "x = layers.Flatten()(x)\n",
        "x = layers.Dense(n_classes)(x)\n",
        "\n",
        "model = keras.Model(input, x)\n"
      ],
      "metadata": {
        "id": "H6tjbtrx7qm4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "U9pjfZSm4kQQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Helper functions"
      ],
      "metadata": {
        "id": "hUh7o3AuwNK8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def walk_dirs(root_dir, posix_path=True, verbose=False):\n",
        "    \"List directories of each file\"\n",
        "    dirs = []\n",
        "    for foldername, subfolders, filenames in os.walk(root_dir):\n",
        "        for filename in filenames:\n",
        "            file_path = os.path.join(foldername, filename)\n",
        "            if posix_path: dirs.append(PosixPath(file_path))\n",
        "            else: dirs.append(file_path)\n",
        "            if verbose: print('File path:', file_path)\n",
        "\n",
        "    return dirs\n",
        "\n",
        "\n",
        "def prepare_dirs(root_lower_GI=str, root_upper_GI=str, posix_path=bool, shuffle=bool, seed=int, verbose=bool):\n",
        "\n",
        "    \"Returns pairs of (file_directory, class_value)\"\n",
        "\n",
        "    # Get complete directories of each file\n",
        "    dirs_lower_GI = walk_dirs(root_lower_GI, posix_path, verbose)\n",
        "    dirs_upper_GI = walk_dirs(root_upper_GI, posix_path, verbose)\n",
        "\n",
        "    # Create class values\n",
        "    class_lower_GI = [0 for i in range(len(dirs_lower_GI))] # [0,0, ....., 0]\n",
        "    class_upper_GI = [1 for i in range(len(dirs_lower_GI))] # [1,1, ....., 1]\n",
        "\n",
        "    # Create (dirs, class) pair\n",
        "    lower_GI_pairs = list(zip(dirs_lower_GI, class_lower_GI))\n",
        "    upper_GI_pairs = list(zip(dirs_upper_GI, class_upper_GI))\n",
        "\n",
        "    # Shuffling\n",
        "    if shuffle:\n",
        "        random.seed(seed)\n",
        "        random.shuffle(lower_GI_pairs)\n",
        "        random.shuffle(upper_GI_pairs)\n",
        "\n",
        "    return lower_GI_pairs, upper_GI_pairs"
      ],
      "metadata": {
        "id": "L9l9cdfswMsP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data directory"
      ],
      "metadata": {
        "id": "fgZAy8ORUdqt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "root_lower_GI = '/content/drive/MyDrive/Video_classification/dataset/HyperKvasir/lower-gi-tract/'\n",
        "root_upper_GI = '/content/drive/MyDrive/Video_classification/dataset/HyperKvasir/upper-gi-tract/'\n",
        "\n",
        "n_sample_per_GI = 60 # no. of samples for each GI.\n",
        "posix_path = True\n",
        "shuffle = True\n",
        "verbose = False\n",
        "\n",
        "lower_GI_pairs, upper_GI_pairs = prepare_dirs(root_lower_GI, root_upper_GI, posix_path, shuffle, seed, verbose)\n",
        "\n",
        "# Select no. of samples that will be used\n",
        "selected_lower_GI_pairs = lower_GI_pairs[:n_sample_per_GI]\n",
        "selected_upper_GI_pairs = upper_GI_pairs[:n_sample_per_GI]\n",
        "\n",
        "# Split into train, val, and test. Will not do shuffling now. Shuffling will be\n",
        "# done for training pairs in the training dataloader. Note that lower GI and\n",
        "# upper GI are themselves already shuffled.\n",
        "n_train = int(n_sample_per_GI * 0.70)    # 70% for training\n",
        "n_val_test = n_sample_per_GI - n_train\n",
        "n_val = int(n_val_test * 0.5)            # 15% for validation\n",
        "n_test = n_val_test - n_val              # 15% for inference\n",
        "\n",
        "train_pairs = selected_lower_GI_pairs[:n_train] + selected_upper_GI_pairs[:n_train]\n",
        "val_pairs = selected_lower_GI_pairs[n_train:n_train+n_val] + selected_upper_GI_pairs[n_train:n_train+n_val]\n",
        "test_pairs = selected_lower_GI_pairs[n_train+n_val:] + selected_upper_GI_pairs[n_train+n_val:]\n",
        "\n",
        "print('Total lower GI pairs:', len(lower_GI_pairs))\n",
        "print('Total upper GI pairs:', len(upper_GI_pairs))\n",
        "print('Total training pairs:', len(train_pairs))\n",
        "print('Total validation pairs:', len(val_pairs))\n",
        "print('Total test pairs:', len(test_pairs))"
      ],
      "metadata": {
        "id": "JTxDIB7gv_Ln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataloader"
      ],
      "metadata": {
        "id": "sVf6heVa3ZoY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the dataloader\n",
        "fg = ucf.FrameGeneratorMKD(train_pairs, 10, training=True)\n",
        "\n",
        "frames, label = next(fg())\n",
        "\n",
        "print(f\"Shape: {frames.shape}\")\n",
        "print(f\"Label: {label}\")"
      ],
      "metadata": {
        "id": "mT_YBfMS3cAZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Output signature\n",
        "output_signature = (tf.TensorSpec(shape = (None, None, None, 3), dtype = tf.float32),\n",
        "                    tf.TensorSpec(shape = (), dtype = tf.int16))\n",
        "\n",
        "# Create the training set\n",
        "train_ds = tf.data.Dataset.from_generator(ucf.FrameGeneratorMKD(train_pairs, 10, training=True),\n",
        "                                          output_signature = output_signature)\n",
        "\n",
        "# Create the validation set\n",
        "val_ds = tf.data.Dataset.from_generator(ucf.FrameGeneratorMKD(val_pairs, 10),\n",
        "                                        output_signature = output_signature)\n",
        "\n",
        "# Print the shapes of the data\n",
        "train_frames, train_labels = next(iter(train_ds))\n",
        "print(f'Shape of training set of frames: {train_frames.shape}')\n",
        "print(f'Shape of training labels: {train_labels.shape}')\n",
        "\n",
        "val_frames, val_labels = next(iter(val_ds))\n",
        "print(f'Shape of validation set of frames: {val_frames.shape}')\n",
        "print(f'Shape of validation labels: {val_labels.shape}')"
      ],
      "metadata": {
        "id": "17LBKYnp3qyi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use buffered prefetching such that you can yield data from the disk without having I/O become blocking. Two important functions to use while loading data are:\n",
        "\n",
        "Dataset.cache: keeps the sets of frames in memory after they're loaded off the disk during the first epoch. This function ensures that the dataset does not become a bottleneck while training your model. If your dataset is too large to fit into memory, you can also use this method to create a performant on-disk cache.\n",
        "\n",
        "Dataset.prefetch: overlaps data preprocessing and model execution while training. Refer to Better performance with the tf.data for details."
      ],
      "metadata": {
        "id": "G916_mfI4dCS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size = AUTOTUNE)\n",
        "val_ds = val_ds.cache().shuffle(1000).prefetch(buffer_size = AUTOTUNE)"
      ],
      "metadata": {
        "id": "EiBN2LBy38Gw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To prepare the data to be fed into the model, use batching. Notice that when working with video data, such as AVI files, the data should be shaped as a five dimensional object. These dimensions are as follows: [batch_size, number_of_frames, height, width, channels]. In comparison, an image would have four dimensions: [batch_size, height, width, channels]. The image below is an illustration of how the shape of video data is represented."
      ],
      "metadata": {
        "id": "i1Rf_QTO4eu_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = train_ds.batch(2)\n",
        "val_ds = val_ds.batch(2)\n",
        "\n",
        "train_frames, train_labels = next(iter(train_ds))\n",
        "print(f'Shape of training set of frames: {train_frames.shape}')\n",
        "print(f'Shape of training labels: {train_labels.shape}')\n",
        "\n",
        "val_frames, val_labels = next(iter(val_ds))\n",
        "print(f'Shape of validation set of frames: {val_frames.shape}')\n",
        "print(f'Shape of validation labels: {val_labels.shape}')"
      ],
      "metadata": {
        "id": "za68s_DN3b3I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the model\n",
        "model.build(train_frames)"
      ],
      "metadata": {
        "id": "X6o2jnC67vB4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the model\n",
        "keras.utils.plot_model(model, expand_nested=True, dpi=60, show_shapes=True)"
      ],
      "metadata": {
        "id": "W_FRrgrFDNQ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the model"
      ],
      "metadata": {
        "id": "6cUGNdwzDdje"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create checkpoint directory\n",
        "checkpoint_loc = os.path.join('/content/drive/MyDrive/Video_classification/checkpoints', model_name)\n",
        "os.makedirs(checkpoint_loc, exist_ok=True) # if the directory does not exist, then create it.\n",
        "\n",
        "# checkpoint_path = os.path.join(checkpoint_loc, \"cp-{epoch:04d}.ckpt\") # use this checkpoint path when save_best_only is False\n",
        "# checkpoint_path = os.path.join(checkpoint_loc, \"best_model.ckpt\") # # use this checkpoint path when save_best_only is True\n",
        "\n",
        "checkpoint_path = os.path.join(checkpoint_loc, \"best_model.h5\") # # use this checkpoint path when save_best_only is True"
      ],
      "metadata": {
        "id": "xIKjhT1DRmOb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Callbacks\n",
        "callbacks = [\n",
        "    # EarlyStopping(monitor='', patience=5, verbose=1),\n",
        "    ReduceLROnPlateau(factor=0.1,\n",
        "                      monitor='val_loss',\n",
        "                      patience=10,\n",
        "                      min_lr=0.00001,\n",
        "                      verbose=1,\n",
        "                      mode='auto'),\n",
        "    ModelCheckpoint(checkpoint_path,\n",
        "                      monitor = 'val_loss',\n",
        "                      verbose = 1,\n",
        "                      save_best_only=True,\n",
        "                      save_weights_only=True,\n",
        "                      period=1),\n",
        "    # CSVLogger(os.path.join(log_path, datetime.now().strftime('%Y-%m-%d_%H-%M-%S') + '.csv'), separator=',', append=True),\n",
        "    # TerminateOnNaN()\n",
        "]"
      ],
      "metadata": {
        "id": "5L6fKo0dRvlj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1662e41b-2812-4d66-8d47-56e45c66f629"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              optimizer = keras.optimizers.Adam(learning_rate = 0.0001),\n",
        "              metrics = ['accuracy'])"
      ],
      "metadata": {
        "id": "EvwOQgcLDWyf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(x = train_ds,\n",
        "                    validation_data = val_ds,\n",
        "                    epochs = 50,\n",
        "                    callbacks=callbacks,\n",
        "                    )"
      ],
      "metadata": {
        "id": "L-erRf9iDfXg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create plots of the loss and accuracy on the training and validation sets:"
      ],
      "metadata": {
        "id": "GMT71y9PDqsm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_history(history, full_dir):\n",
        "  \"\"\"\n",
        "    Plotting training and validation learning curves.\n",
        "\n",
        "    Args:\n",
        "      history: model history with all the metric measures\n",
        "  \"\"\"\n",
        "  fig, (ax1, ax2) = plt.subplots(2,1, figsize=(5, 8))\n",
        "\n",
        "  # Plot loss\n",
        "  ax1.set_title('Loss')\n",
        "  ax1.plot(history.history['loss'], label = 'train')\n",
        "  ax1.plot(history.history['val_loss'], label = 'test')\n",
        "  ax1.set_ylabel('Loss')\n",
        "\n",
        "  # Determine upper bound of y-axis\n",
        "  max_loss = max(history.history['loss'] + history.history['val_loss'])\n",
        "\n",
        "  ax1.set_ylim([0, np.ceil(max_loss)])\n",
        "  ax1.set_xlabel('Epoch')\n",
        "  ax1.legend(['Train', 'Validation'])\n",
        "\n",
        "  # Plot accuracy\n",
        "  ax2.set_title('Accuracy')\n",
        "  ax2.plot(history.history['accuracy'],  label = 'train')\n",
        "  ax2.plot(history.history['val_accuracy'], label = 'test')\n",
        "  ax2.set_ylabel('Accuracy')\n",
        "  ax2.set_ylim([0, 1])\n",
        "  ax2.set_xlabel('Epoch')\n",
        "  ax2.legend(['Train', 'Validation'])\n",
        "\n",
        "  plt.savefig(full_dir)\n",
        "  # plt.show()\n",
        "\n",
        "\n",
        "root = '/content/drive/MyDrive/Video_classification/'\n",
        "save_fig_dir = os.path.join(root, \"plots\")\n",
        "os.makedirs(save_fig_dir, exist_ok=True)\n",
        "full_dir = os.path.join(save_fig_dir, model_name + '.png')\n",
        "\n",
        "plot_history(history, full_dir)"
      ],
      "metadata": {
        "id": "-k57AKYUDlsW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save the model"
      ],
      "metadata": {
        "id": "RyfG7oIp7Yvf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model.save_weights(os.path.join(checkpoint_loc, \"last_model.h5\"))"
      ],
      "metadata": {
        "id": "ssUGIOnb7bHP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inference"
      ],
      "metadata": {
        "id": "zr7niRSQD3Z3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load model"
      ],
      "metadata": {
        "id": "B8trK7aRW0_q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Load model from checkpoint\n",
        "# from keras.models import load_model\n",
        "\n",
        "# # Checkpoint directory\n",
        "# # checkpoint_path = os.path.join('/content/drive/MyDrive/MS_sdsmt/checkpoints/LSTM_2023-10-09_01-06-20', \"best_model.ckpt\")\n",
        "\n",
        "# model = load_model(checkpoint_path, compile=False)\n",
        "\n",
        "# Uncomment to load from model weights\n",
        "model.load_weights(os.path.join(checkpoint_loc, \"best_model.h5\"))"
      ],
      "metadata": {
        "id": "Ap-FTiW7W0Jy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_signature = (tf.TensorSpec(shape=(None, None, None, 3), dtype=tf.float32),\n",
        "                    tf.TensorSpec(shape=(), dtype=tf.int16))\n",
        "\n",
        "# Create the test set using a separate generator or method\n",
        "test_ds = tf.data.Dataset.from_generator(ucf.FrameGeneratorMKD(test_pairs, 10, training=False),\n",
        "                                        output_signature=output_signature)\n",
        "\n",
        "# Apply similar processing steps as the training set\n",
        "test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE) # omitting shuffle\n",
        "\n",
        "test_ds = test_ds.batch(2)"
      ],
      "metadata": {
        "id": "LhChk01DHJIu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate"
      ],
      "metadata": {
        "id": "fUPFsOrp0jvB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(test_ds, return_dict=True)"
      ],
      "metadata": {
        "id": "qMbXzzeNDy-v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To visualize model performance further, use a confusion matrix. The confusion matrix allows you to assess the performance of the classification model beyond accuracy. In order to build the confusion matrix for this multi-class classification problem, get the actual values in the test set and the predicted values."
      ],
      "metadata": {
        "id": "gEM-zBqeIwrz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_actual_predicted_labels(dataset):\n",
        "  \"\"\"\n",
        "    Create a list of actual ground truth values and the predictions from the model.\n",
        "\n",
        "    Args:\n",
        "      dataset: An iterable data structure, such as a TensorFlow Dataset, with features and labels.\n",
        "\n",
        "    Return:\n",
        "      Ground truth and predicted values for a particular dataset.\n",
        "  \"\"\"\n",
        "  actual = [labels for _, labels in dataset.unbatch()]\n",
        "  predicted = model.predict(dataset)\n",
        "\n",
        "  actual = tf.stack(actual, axis=0)\n",
        "  predicted = tf.concat(predicted, axis=0)\n",
        "  predicted = tf.argmax(predicted, axis=1)\n",
        "\n",
        "  return actual, predicted"
      ],
      "metadata": {
        "id": "7jWxCOHZIqiu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "actual, predicted = get_actual_predicted_labels(test_ds)"
      ],
      "metadata": {
        "id": "AhLFByc_-r9b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Actual:\\n', actual)\n",
        "print('Predicted:\\n', predicted)"
      ],
      "metadata": {
        "id": "MnVj7Y4g-tkk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create test directory to save test plots\n",
        "test_save_dir = os.path.join(root, \"predictions\")\n",
        "os.makedirs(test_save_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "sADJi7fDPCs5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plot confusion matrix"
      ],
      "metadata": {
        "id": "hiIOuPoGLDgs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_confusion_matrix(actual, predicted, full_dir):\n",
        "  plt.figure(figsize=(8, 6))\n",
        "  cm = tf.math.confusion_matrix(actual, predicted)\n",
        "  sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False,\n",
        "              xticklabels=['Predicted 0', 'Predicted 1'],\n",
        "              yticklabels=['Actual 0', 'Actual 1'])\n",
        "  plt.xlabel('Predicted label')\n",
        "  plt.ylabel('True label')\n",
        "  plt.title('Confusion Matrix')\n",
        "  plt.savefig(full_dir)\n",
        "  # plt.show()\n",
        "\n",
        "full_dir = os.path.join(test_save_dir, 'conf_mat_' + model_name + '.png')\n",
        "plot_confusion_matrix(actual, predicted, full_dir)"
      ],
      "metadata": {
        "id": "2BbElYUqI1o3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Classification report"
      ],
      "metadata": {
        "id": "b2RMfySWMFl4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "actual = actual.cpu().numpy().tolist()\n",
        "predicted = predicted.cpu().numpy().tolist()"
      ],
      "metadata": {
        "id": "qo9tHakDRXPz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
        "from sklearn.metrics import roc_curve, RocCurveDisplay, auc\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import pandas as pd\n",
        "\n",
        "# ============ CM ======================\n",
        "acc = accuracy_score(actual, predicted)\n",
        "print('Accuracy:', acc*100)\n",
        "\n",
        "cm = confusion_matrix(actual, predicted)\n",
        "\n",
        "#---------------Generate confusion matrix---------------#\n",
        "cr = classification_report(actual, predicted,\n",
        "                           labels=[0, 1], output_dict=True)\n",
        "\n",
        "df_cm = pd.DataFrame(cr).transpose() # convert confusion matrix to dataframe\n",
        "print(df_cm)\n",
        "df_cm.to_excel(os.path.join(test_save_dir, 'classification_report_' + model_name + \".xlsx\"))"
      ],
      "metadata": {
        "id": "RJeVxJu-MMx6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute ROC curve and ROC area for each class\n",
        "fpr, tpr, _ = roc_curve(actual, predicted)\n",
        "roc_auc = roc_auc_score(actual, predicted)\n",
        "\n",
        "# Plot ROC curve with AUC value annotation\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "plt.legend(loc='lower right')\n",
        "\n",
        "plt.savefig(os.path.join(test_save_dir, 'roc_' + model_name + \".png\" ))\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "32KxZiYWSlN9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate precision and recall using tensorflow. The precision and recall values for each class can also be calculated using a confusion matrix."
      ],
      "metadata": {
        "id": "e2mg_rOOXovi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_classification_metrics(y_actual, y_pred, labels):\n",
        "  \"\"\"\n",
        "    Calculate the precision and recall of a classification model using the ground truth and\n",
        "    predicted values.\n",
        "\n",
        "    Args:\n",
        "      y_actual: Ground truth labels.\n",
        "      y_pred: Predicted labels.\n",
        "      labels: List of classification labels.\n",
        "\n",
        "    Return:\n",
        "      Precision and recall measures.\n",
        "  \"\"\"\n",
        "  cm = tf.math.confusion_matrix(y_actual, y_pred)\n",
        "  tp = np.diag(cm) # Diagonal represents true positives\n",
        "  precision = dict()\n",
        "  recall = dict()\n",
        "  for i in range(len(labels)):\n",
        "    col = cm[:, i]\n",
        "    fp = np.sum(col) - tp[i] # Sum of column minus true positive is false negative\n",
        "\n",
        "    row = cm[i, :]\n",
        "    fn = np.sum(row) - tp[i] # Sum of row minus true positive, is false negative\n",
        "\n",
        "    precision[labels[i]] = tp[i] / (tp[i] + fp) # Precision\n",
        "\n",
        "    recall[labels[i]] = tp[i] / (tp[i] + fn) # Recall\n",
        "\n",
        "  return precision, recall"
      ],
      "metadata": {
        "id": "djaHNvK4XmrP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "precision, recall = calculate_classification_metrics(actual, predicted, labels=[0,1]) # Test dataset"
      ],
      "metadata": {
        "id": "WGzLrRl1Xr8q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reference: https://www.tensorflow.org/tutorials/video/video_classification"
      ],
      "metadata": {
        "id": "jHtNsBEr2xNH"
      }
    }
  ]
}